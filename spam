
# coding: utf-8

# In[ ]:

import numpy as np
import pandas as pd
#from sklearn.model_selection import train_test_split
from sklearn.cross_validation import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.model_selection import cross_val_score
import seaborn as sns; sns.set()
from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB
from sklearn.metrics import accuracy_score,confusion_matrix,f1_score
import matplotlib.pyplot as plt
get_ipython().magic('matplotlib inline')
import nltk
from nltk.corpus import stopwords
import warnings
warnings.filterwarnings('ignore')


# In[ ]:

message = [line.rstrip() for line in open('spam.csv')]
print(len(message))


# In[ ]:

for message_no,message in enumerate(message[:10]):
    print(message_no,message)
    print('\n')


# In[ ]:

# Reading the data from spam.csv file.
data=pd.read_csv('spam.csv',encoding='latin-1')
data.drop(["Unnamed: 2", "Unnamed: 3", "Unnamed: 4"], axis = 1, inplace = True)  # dropped columns which are not required.
data = data.rename(columns = {"v1":"classlabels","v2":"message"})
data.head()


# In[ ]:

data.describe()


# In[ ]:

data.groupby('classlabels').describe()


# In[ ]:

#Create a new column to detect how long the text messages are:

data['length']=data['message'].apply(len)
data.head()


# # Data Visualization

# In[ ]:

data['length'].plot(bins=50,kind='hist')


# In[ ]:

data.length.describe()


# In[ ]:

# To find maximum length of the message, which is 910 characters.
data[data['length']==910]['message'].iloc[0]


# # Text Pre-processing
Our main issue with our data is that it is all in text format (strings). The classification algorithms that we've learned about so far will need some sort of numerical feature vector in order to perform the classification task. There are actually many methods to convert a corpus to a vector format. The simplest is the the bag-of-words approach, where each unique word in a text will be represented by one number.

we'll convert the raw messages (sequence of characters) into vectors (sequences of numbers).

As a first step, let's write a function that will split a message into its individual words and return a list. We'll also remove very common words, ('the', 'a', etc..). To do this we will take advantage of the NLTK library. It's pretty much the standard library in Python for processing text and has a lot of useful features. We'll only use some of the basic ones here.

Let's create a function that will process the string in the message column, then we can just use apply() in pandas do process all the text in the DataFrame.

First removing punctuation. We can just take advantage of Python's built-in string library to get a quick list of all the possible punctuation:
# In[ ]:

import string
mess = 'sample message!...'
nopunc=[char for char in mess if char not in string.punctuation]
nopunc=''.join(nopunc)
print(nopunc)


# In[ ]:

nopunc.split()


# In[ ]:

def text_process(mess):
    nopunc =[char for char in mess if char not in string.punctuation]
    nopunc=''.join(nopunc)
    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]


# In[ ]:

data.head()


# In[ ]:


data['message'].head(5).apply(text_process)


# In[ ]:

data.head()


# # Continuing Normalization 
# 
# Below code to convert list of words to an actual vector using Scikit_learn.
# We will convert each message , presented as a list of tokens or lemmas as above, into a vector taht ML models can understand.
# 
# Here we will do 3 steps using bags-of-words model:
# 1. Count how many times does a word occur in each message ( known as term frequency )
# 2. Weigh the counts, so that frequent tokens get lower weight ( inverse document frequency )
# 2. Normalize the vectors to unit length, to abstract from the original text length ( L2 norm )
# 
# Scikit Learn's CountVectorizer model, wull convert a collection of text documents to a matrix of token counts.
# 

# In[ ]:

#For each text message and get its bag-of-words counts as a vector.

bow_transformer = CountVectorizer(analyzer=text_process).fit(data['message'])
print(len(bow_transformer.vocabulary_))


# In[ ]:

message1=data['message'][1]
print(message1)


# In[ ]:

#Vector representation

bow1=bow_transformer.transform([message1])
print(bow1)
print(bow1.shape)

This means that there are seven unique words in message number 4 (after removing common stop words).
Two of them appear twice, the rest only once. 
Let's go ahead and check and confirm which ones appear twice:
# In[ ]:

print(bow_transformer.get_feature_names()[7600])
print(bow_transformer.get_feature_names()[10952])


# In[ ]:

#Using .transform on our Bag-of-words transformed object and transform the entire DataFrame of messages. 
#Now, check how bag-of-words count for the entire dataframe of msssages.

messages_bow = bow_transformer.transform(data['message'])


# In[ ]:

print(messages_bow);


# In[ ]:

print('Shape of Sparse Matrix: ',messages_bow.shape)
print('Amount of non-zero occurences:',messages_bow.nnz)


# In[ ]:

'''
In numerical analysis and scientific computing, a sparse matrix or sparse array is a matrix in which most of the elements 
are zero. By contrast, if most of the elements are nonzero, then the matrix is considered dense. The number of zero-valued 
elements divided by the total number of elements (e.g., m × n for an m × n matrix) is called the sparsity of the matrix 
(which is equal to 1 minus the density of the matrix). Using those definitions, a matrix will be sparse when its sparsity 
is greater than 0.5.
'''

sparsity =(100.0 * messages_bow.nnz/(messages_bow.shape[0]*messages_bow.shape[1]))
print('sparsity:{}'.format(round(sparsity)))


# TF-IDF stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining.This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus.
# 
# Typically, the tf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.
# 
# TF: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:
# 
# TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).
# 
# IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as "is", "of", and "that", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:
# 
# IDF(t) = log_e(Total number of documents / Number of documents with term t in it).
# 
# Example:
# 
# Consider a document containing 100 words wherein the word cat appears 3 times.
# 
# The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.
# 

# In[ ]:

#Using scikit-learn TF-IDF (TfidfTransformer), you can do term weighting and normalization.

from sklearn.feature_extraction.text import TfidfTransformer
tfidf_transformer=TfidfTransformer().fit(messages_bow)
tfidf4 = tfidf_transformer.transform(bow1)
print(tfidf4)


# In[ ]:

#To check IDF for the words "lar" and "wif"

print(tfidf_transformer.idf_[bow_transformer.vocabulary_['lar']])
print(tfidf_transformer.idf_[bow_transformer.vocabulary_['wif']])


# In[ ]:

messages_tfidf=tfidf_transformer.transform(messages_bow)
print(messages_tfidf.shape)


# # Training a model

# In[ ]:

#from sklearn.naive_bayes import MultinomialNB
spam_detect_model = MultinomialNB().fit(messages_tfidf,data['classlabels'])


# In[ ]:

print('predicted:',spam_detect_model.predict(tfidf4)[0])
print('expected:',data.classlabels[3])


# # Model Evaluation

# In[ ]:

all_predictions = spam_detect_model.predict(messages_tfidf)
print(all_predictions)


# In[ ]:

#classification_report and cnfusion matrix

from sklearn.metrics import classification_report,confusion_matrix
print(classification_report(data['classlabels'],all_predictions))
print(confusion_matrix(data['classlabels'],all_predictions))


# # Train Test Split
Split the data into a train and test dataset, where the model sees the training data during its model fitting and parameter tuning. 
Test data is never seen by the model and then evaluate the mode on test data to represent predictive performance.
# In[ ]:


#from sklearn.model_selection import train_test_split
msg_train,msg_test,label_train,label_test = train_test_split(data['message'],data['classlabels'],test_size=0.2)


# In[ ]:

print(len(msg_train),len(msg_test),len(label_train),len(label_test))


# # MultinomialNB Classifier 

# In[ ]:

#Scikit learn pipeline capabilities to store a pipeline workflow and will us to set up all the transformations that we will do 
# the data for future use.

from sklearn.pipeline import Pipeline
model = Pipeline([
   ( 'bow',CountVectorizer(analyzer=text_process)),
    ('tfidf',TfidfTransformer()),
    ('classifier',MultinomialNB()),
])


# In[ ]:

#MultinomialNB = MultinomialNB()
get_ipython().magic('time model.fit(msg_train,label_train)')


# In[ ]:

predictions_labels = model.predict(msg_test)


# In[ ]:

print (predictions_labels)


# In[ ]:

print(label_test);


# In[ ]:

class_report = classification_report(label_test,predictions_labels)
conf_mat = confusion_matrix(label_test,predictions_labels)
print(classification_report(label_test,predictions_labels))
print(confusion_matrix(label_test,predictions_labels))


# In[ ]:

from sklearn import metrics
metrics.accuracy_score(label_test,predictions_labels)*100


# In[ ]:

print(cross_val_score(model, msg_train,label_train, cv=10))


# # GaussianNB Classifier

# In[ ]:



